{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5e3038",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "Import the Python modules used throughout the notebook. Make sure you have already installed the packages listed in the README (pandas, numpy, mutagen, unidecode)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4947393f",
   "metadata": {},
   "source": [
    "### Package bootstrap\n",
    "Install any missing Python packages required by this workflow so the import cell succeeds even on a fresh environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd91dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "REQUIRED_PACKAGES = {\n",
    "    \"pandas\": \"pandas\",\n",
    "    \"numpy\": \"numpy\",\n",
    "    \"mutagen\": \"mutagen\",\n",
    "    \"unidecode\": \"Unidecode\"\n",
    "}\n",
    "\n",
    "for module_name, install_name in REQUIRED_PACKAGES.items():\n",
    "    try:\n",
    "        importlib.import_module(module_name)\n",
    "    except ImportError:\n",
    "        print(f\"Installing missing dependency: {install_name}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", install_name])\n",
    "print(\"Dependency check complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ecf47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional\n",
    "\n",
    "import math\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mutagen import File as MutagenFile\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5638e",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "Define key directories (relative to the repository root) and ensure the output folder for reports exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4770fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust these paths if you relocate folders.\n",
    "REPO_ROOT = Path.cwd()\n",
    "DOWNLOAD_ROOT = REPO_ROOT / \"Downloaded\"\n",
    "SPOTIFY_PLAYLISTS = REPO_ROOT / \"spotify_playlists\"\n",
    "SHOPPING_LIST_DIR = REPO_ROOT / \"shopping_lists\"\n",
    "LIBRARY_INDEX_CSV = REPO_ROOT / \"library_index.csv\"\n",
    "ADD_ROOT = REPO_ROOT / \"Add\"\n",
    "\n",
    "SHOPPING_LIST_DIR.mkdir(exist_ok=True)\n",
    "ADD_ROOT.mkdir(exist_ok=True)\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Download library: {DOWNLOAD_ROOT}\")\n",
    "print(f\"Spotify playlist CSVs: {SPOTIFY_PLAYLISTS}\")\n",
    "print(f\"Shopping/output directory: {SHOPPING_LIST_DIR}\")\n",
    "print(f\"Add drop directory: {ADD_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d70d559",
   "metadata": {},
   "source": [
    "## 2. Helper functions\n",
    "Canonicalization helpers keep matching consistent between Spotify exports and local audio metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31587618",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_ALNUM = re.compile(r\"[^a-z0-9]+\")\n",
    "FEAT_PATTERN = re.compile(r\"\\(feat\\..*?\\)\", re.IGNORECASE)\n",
    "REMIX_PATTERN = re.compile(r\"-\\s*(remaster(ed)?|remix|edit|mix).*\", re.IGNORECASE)\n",
    "AUDIO_EXTENSIONS = {'.mp3', '.flac', '.m4a', '.aac', '.ogg', '.wav', '.aiff'}\n",
    "INVALID_PATH_CHARS = re.compile(r\"[<>:\\\"/\\\\|?*]\")\n",
    "\n",
    "\n",
    "def canonicalize_string(value: Optional[str]) -> str:\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    normalized = unidecode(str(value))\n",
    "    normalized = FEAT_PATTERN.sub(\"\", normalized)\n",
    "    normalized = REMIX_PATTERN.sub(\"\", normalized)\n",
    "    normalized = normalized.lower()\n",
    "    normalized = NON_ALNUM.sub(\" \", normalized)\n",
    "    normalized = normalized.strip()\n",
    "    return re.sub(r\"\\s+\", \" \", normalized)\n",
    "\n",
    "\n",
    "def safe_path_component(value: Optional[str], fallback: str = \"Unknown\") -> str:\n",
    "    candidate = str(value).strip() if value else fallback\n",
    "    candidate = unidecode(candidate)\n",
    "    candidate = INVALID_PATH_CHARS.sub(\"\", candidate)\n",
    "    candidate = candidate.replace('/', '-').replace('\\\\', '-')\n",
    "    candidate = candidate.strip()\n",
    "    return candidate or fallback\n",
    "\n",
    "\n",
    "def primary_artist(artists_field: Optional[str]) -> str:\n",
    "    if not artists_field or not isinstance(artists_field, str):\n",
    "        return \"\"\n",
    "    first = artists_field.split(';')[0]\n",
    "    return first.strip()\n",
    "\n",
    "\n",
    "def friendly_playlist_name(csv_path: Path) -> str:\n",
    "    name = csv_path.stem.replace('_', ' ')\n",
    "    return name.strip()\n",
    "\n",
    "\n",
    "def duration_ms_from_audio(audio_obj) -> Optional[int]:\n",
    "    if audio_obj and audio_obj.info and getattr(audio_obj.info, 'length', None):\n",
    "        return int(round(audio_obj.info.length * 1000))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d8f47",
   "metadata": {},
   "source": [
    "## 3. Process new additions\n",
    "Move fresh MP3 downloads from the staging `Add/` folder into the canonical `Downloaded/Artist/Album/Title.mp3` structure before scanning the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e447a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_IMPORT_SUFFIXES = {'.mp3'}\n",
    "\n",
    "\n",
    "def tag_value(tag) -> Optional[str]:\n",
    "    if tag is None:\n",
    "        return None\n",
    "    if hasattr(tag, 'text') and getattr(tag, 'text'):\n",
    "        return str(tag.text[0])\n",
    "    if isinstance(tag, (list, tuple)) and tag:\n",
    "        return str(tag[0])\n",
    "    if isinstance(tag, str):\n",
    "        return tag\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_new_additions(add_root: Path, download_root: Path) -> pd.DataFrame:\n",
    "    if not add_root.exists():\n",
    "        print(f\"Add directory not found: {add_root}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    mp3_files = sorted(add_root.rglob('*.mp3'))\n",
    "    if not mp3_files:\n",
    "        print(f\"No new MP3 files found under {add_root}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    actions = []\n",
    "    for src_path in mp3_files:\n",
    "        rel_src = src_path.relative_to(add_root)\n",
    "        if not src_path.exists():\n",
    "            message = \"Source file missing before processing\"\n",
    "            print(f\"Skipping {rel_src}: {message}\")\n",
    "            actions.append({\n",
    "                'source': rel_src.as_posix(),\n",
    "                'destination': None,\n",
    "                'status': 'skipped_missing_source',\n",
    "                'message': message\n",
    "            })\n",
    "            continue\n",
    "        try:\n",
    "            audio = MutagenFile(src_path)\n",
    "        except Exception as exc:\n",
    "            message = f\"Unable to read metadata: {exc}\"\n",
    "            print(f\"Skipping {rel_src}: {message}\")\n",
    "            actions.append({\n",
    "                'source': rel_src.as_posix(),\n",
    "                'destination': None,\n",
    "                'status': 'error_read',\n",
    "                'message': message\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        tags = getattr(audio, 'tags', None) if audio else None\n",
    "        artist_raw = tag_value(tags.get('TPE1')) if tags else None\n",
    "        artist_raw = artist_raw or tag_value(tags.get('artist')) if tags else artist_raw\n",
    "        album_raw = tag_value(tags.get('TALB')) if tags else None\n",
    "        album_raw = album_raw or tag_value(tags.get('album')) if tags else album_raw\n",
    "        title_raw = tag_value(tags.get('TIT2')) if tags else None\n",
    "        title_raw = title_raw or tag_value(tags.get('title')) if tags else title_raw\n",
    "\n",
    "        artist_component = safe_path_component(artist_raw, 'Unknown Artist')\n",
    "        album_component = safe_path_component(album_raw, 'Unknown Album')\n",
    "        title_value = title_raw or src_path.stem\n",
    "        title_component = safe_path_component(title_value, src_path.stem)\n",
    "        title_canonical = canonicalize_string(title_value)\n",
    "\n",
    "        dest_dir = download_root / artist_component / album_component\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        duplicate_path = None\n",
    "        for existing_path in dest_dir.glob('*'):\n",
    "            if not existing_path.is_file() or existing_path.suffix.lower() not in AUDIO_EXTENSIONS:\n",
    "                continue\n",
    "            try:\n",
    "                existing_audio = MutagenFile(existing_path)\n",
    "            except Exception:\n",
    "                existing_audio = None\n",
    "            existing_tags = getattr(existing_audio, 'tags', None) if existing_audio else None\n",
    "            existing_title = None\n",
    "            if existing_tags:\n",
    "                existing_title = tag_value(existing_tags.get('TIT2'))\n",
    "                if not existing_title:\n",
    "                    existing_title = tag_value(existing_tags.get('title'))\n",
    "            if not existing_title:\n",
    "                existing_title = existing_path.stem\n",
    "            existing_canonical = canonicalize_string(existing_title)\n",
    "            if existing_canonical == title_canonical:\n",
    "                duplicate_path = existing_path\n",
    "                break\n",
    "\n",
    "        if duplicate_path is not None:\n",
    "            message = f\"Title already exists: {duplicate_path.relative_to(download_root)}\"\n",
    "            print(f\"Skipping {rel_src}: {message}\")\n",
    "            actions.append({\n",
    "                'source': rel_src.as_posix(),\n",
    "                'destination': duplicate_path.relative_to(download_root).as_posix(),\n",
    "                'status': 'skipped_duplicate_title',\n",
    "                'message': message\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        dest_path = dest_dir / f\"{title_component}{src_path.suffix.lower()}\"\n",
    "\n",
    "        if dest_path.exists():\n",
    "            message = f\"Destination already exists: {dest_path}\"\n",
    "            print(f\"Skipping {rel_src}: {message}\")\n",
    "            actions.append({\n",
    "                'source': rel_src.as_posix(),\n",
    "                'destination': dest_path.relative_to(download_root).as_posix(),\n",
    "                'status': 'skipped_exists',\n",
    "                'message': message\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            shutil.move(str(src_path), dest_path)\n",
    "        except FileNotFoundError as exc:\n",
    "            message = f\"Move failed (missing path): {exc}\"\n",
    "            print(f\"Skipping {rel_src}: {message}\")\n",
    "            actions.append({\n",
    "                'source': rel_src.as_posix(),\n",
    "                'destination': dest_path.relative_to(download_root).as_posix(),\n",
    "                'status': 'error_move',\n",
    "                'message': message\n",
    "            })\n",
    "            continue\n",
    "        except OSError as exc:\n",
    "            message = f\"Move failed: {exc}\"\n",
    "            print(f\"Skipping {rel_src}: {message}\")\n",
    "            actions.append({\n",
    "                'source': rel_src.as_posix(),\n",
    "                'destination': dest_path.relative_to(download_root).as_posix(),\n",
    "                'status': 'error_move',\n",
    "                'message': message\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        actions.append({\n",
    "            'source': rel_src.as_posix(),\n",
    "            'destination': dest_path.relative_to(download_root).as_posix(),\n",
    "            'status': 'moved',\n",
    "            'message': 'Moved into library'\n",
    "        })\n",
    "\n",
    "    # Clean up any empty directories left behind in Add/\n",
    "    for folder in sorted(add_root.rglob('*'), reverse=True):\n",
    "        if folder.is_dir():\n",
    "            try:\n",
    "                next(folder.iterdir())\n",
    "            except StopIteration:\n",
    "                folder.rmdir()\n",
    "\n",
    "    result_df = pd.DataFrame(actions)\n",
    "    moved_count = (result_df['status'] == 'moved').sum() if not result_df.empty else 0\n",
    "    skipped_exists_count = (result_df['status'] == 'skipped_exists').sum() if not result_df.empty else 0\n",
    "    duplicate_count = (result_df['status'] == 'skipped_duplicate_title').sum() if not result_df.empty else 0\n",
    "    missing_source_count = (result_df['status'] == 'skipped_missing_source').sum() if not result_df.empty else 0\n",
    "    error_read_count = (result_df['status'] == 'error_read').sum() if not result_df.empty else 0\n",
    "    error_move_count = (result_df['status'] == 'error_move').sum() if not result_df.empty else 0\n",
    "    print(\n",
    "        f\"Processed {len(actions)} files | Moved: {moved_count} | \"\n",
    "        f\"Skipped (already existed): {skipped_exists_count} | \"\n",
    "        f\"Skipped (duplicate title): {duplicate_count} | \"\n",
    "        f\"Skipped (missing source): {missing_source_count} | \"\n",
    "        f\"Errors (metadata read): {error_read_count} | Errors (move): {error_move_count}\"\n",
    "    )\n",
    "    return result_df\n",
    "\n",
    "\n",
    "new_additions_log = process_new_additions(ADD_ROOT, DOWNLOAD_ROOT)\n",
    "if not new_additions_log.empty:\n",
    "    display(new_additions_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b400792",
   "metadata": {},
   "source": [
    "## 4. Scan the downloaded library\n",
    "Create or refresh an auditable `library_index.csv` capturing metadata for every audio file under `Downloaded/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_downloaded_library(download_root: Path) -> pd.DataFrame:\n",
    "    records = []\n",
    "    if not download_root.exists():\n",
    "        print(f\"Download directory not found: {download_root}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for file_path in download_root.rglob('*'):\n",
    "        if not file_path.is_file() or file_path.suffix.lower() not in AUDIO_EXTENSIONS:\n",
    "            continue\n",
    "        try:\n",
    "            audio = MutagenFile(file_path)\n",
    "        except Exception as exc:\n",
    "            print(f\"Failed to read {file_path}: {exc}\")\n",
    "            audio = None\n",
    "\n",
    "        tags = getattr(audio, 'tags', None) if audio else None\n",
    "        artist_tag = None\n",
    "        title_tag = None\n",
    "        album_tag = None\n",
    "\n",
    "        if tags:\n",
    "            artist_tag = tags.get('TPE1') or tags.get('artist')\n",
    "            title_tag = tags.get('TIT2') or tags.get('title')\n",
    "            album_tag = tags.get('TALB') or tags.get('album')\n",
    "\n",
    "        artist_str = str(artist_tag.text[0]) if hasattr(artist_tag, 'text') else (artist_tag if isinstance(artist_tag, str) else None)\n",
    "        title_str = str(title_tag.text[0]) if hasattr(title_tag, 'text') else (title_tag if isinstance(title_tag, str) else None)\n",
    "        album_str = str(album_tag.text[0]) if hasattr(album_tag, 'text') else (album_tag if isinstance(album_tag, str) else None)\n",
    "\n",
    "        # Fallbacks from the path structure\n",
    "        if not artist_str:\n",
    "            artist_str = file_path.parent.name\n",
    "        if not title_str:\n",
    "            title_str = file_path.stem\n",
    "\n",
    "        records.append({\n",
    "            'file_path': file_path.relative_to(download_root).as_posix(),\n",
    "            'artist_raw': artist_str,\n",
    "            'title_raw': title_str,\n",
    "            'album_raw': album_str,\n",
    "            'artist_canonical': canonicalize_string(artist_str),\n",
    "            'title_canonical': canonicalize_string(title_str),\n",
    "            'duration_ms': duration_ms_from_audio(audio)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    if not df.empty:\n",
    "        df.sort_values(['artist_canonical', 'title_canonical', 'file_path'], inplace=True)\n",
    "    return df\n",
    "\n",
    "library_index = scan_downloaded_library(DOWNLOAD_ROOT)\n",
    "print(f\"Indexed {len(library_index):,} local tracks\")\n",
    "if not library_index.empty:\n",
    "    library_index.to_csv(LIBRARY_INDEX_CSV, index=False)\n",
    "    display(library_index.head())\n",
    "else:\n",
    "    print('Library index is empty – check DOWNLOAD_ROOT or file extensions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed9e1e8",
   "metadata": {},
   "source": [
    "## 5. Load Spotify playlist exports\n",
    "Combine all CSV files in `spotify_playlists/` into a single DataFrame with helpful flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f7a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spotify_playlists(csv_root: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    if not csv_root.exists():\n",
    "        print(f\"Spotify playlist directory not found: {csv_root}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    csv_files = sorted(csv_root.glob('*.csv'))\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {csv_root}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "        except Exception as exc:\n",
    "            print(f\"Failed to read {csv_file}: {exc}\")\n",
    "            continue\n",
    "        df['playlist_name'] = friendly_playlist_name(csv_file)\n",
    "        df['is_liked'] = csv_file.name.lower() == 'liked_songs.csv'\n",
    "        df['is_top_songs'] = csv_file.name.lower().startswith('your_top_songs_')\n",
    "        rows.append(df)\n",
    "\n",
    "    merged = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "    if merged.empty:\n",
    "        return merged\n",
    "\n",
    "    merged['primary_artist'] = merged['Artist Name(s)'].apply(primary_artist)\n",
    "    merged['artist_canonical'] = merged['primary_artist'].apply(canonicalize_string)\n",
    "    merged['title_canonical'] = merged['Track Name'].apply(canonicalize_string)\n",
    "    return merged\n",
    "\n",
    "spotify_df = load_spotify_playlists(SPOTIFY_PLAYLISTS)\n",
    "print(f\"Loaded {len(spotify_df):,} Spotify rows across {spotify_df['playlist_name'].nunique() if not spotify_df.empty else 0} playlists\")\n",
    "if not spotify_df.empty:\n",
    "    display(spotify_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df321a15",
   "metadata": {},
   "source": [
    "## 6. Match Spotify tracks to the local library\n",
    "Left-join on canonical artist/title keys and filter by duration tolerance where available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DURATION_TOLERANCE_MS = 3000\n",
    "\n",
    "def match_tracks(spotify_df: pd.DataFrame, library_df: pd.DataFrame, duration_tolerance_ms: int = DURATION_TOLERANCE_MS) -> pd.DataFrame:\n",
    "    if spotify_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    if library_df.empty:\n",
    "        result = spotify_df.copy()\n",
    "        result['file_path'] = pd.NA\n",
    "        result['duration_ms_local'] = pd.NA\n",
    "        return result\n",
    "\n",
    "    lib_cols = library_df.rename(columns={'duration_ms': 'duration_ms_local'})\n",
    "    merged = spotify_df.merge(lib_cols, how='left', on=['artist_canonical', 'title_canonical'], suffixes=('_spotify', '_local'))\n",
    "\n",
    "    if 'duration_ms_local' in merged.columns:\n",
    "        mask = merged['duration_ms_local'].notna() & merged['Duration (ms)'].notna()\n",
    "        mismatched = mask & (merged['Duration (ms)'] - merged['duration_ms_local']).abs() > duration_tolerance_ms\n",
    "        merged.loc[mismatched, ['file_path', 'duration_ms_local']] = pd.NA\n",
    "    return merged\n",
    "\n",
    "matched_df = match_tracks(spotify_df, library_index)\n",
    "print(f\"Matched rows: {len(matched_df):,}\")\n",
    "if not matched_df.empty:\n",
    "    have_files = matched_df['file_path'].notna().sum()\n",
    "    print(f\"Tracks already downloaded: {have_files:,}\")\n",
    "    print(f\"Tracks missing locally: {len(matched_df) - have_files:,}\")\n",
    "    display(matched_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1594e5",
   "metadata": {},
   "source": [
    "## 7. Generate a dated shopping list\n",
    "Aggregate missing tracks across playlists and export a timestamped CSV in `shopping_lists/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_shopping_list(matched_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if matched_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    missing = matched_df[matched_df['file_path'].isna()].copy()\n",
    "    if missing.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    grouped = (\n",
    "        missing.groupby(['artist_canonical', 'title_canonical'], as_index=False)\n",
    "        .agg({\n",
    "            'primary_artist': 'first',\n",
    "            'Track Name': 'first',\n",
    "            'Album Name': lambda col: col.dropna().iloc[0] if col.dropna().any() else pd.NA,\n",
    "            'Duration (ms)': 'first',\n",
    "            'playlist_name': lambda col: sorted(set(col)),\n",
    "            'is_liked': 'any',\n",
    "            'is_top_songs': 'any'\n",
    "        })\n",
    "    )\n",
    "    grouped['Playlists_Count'] = grouped['playlist_name'].apply(len)\n",
    "    grouped['Playlists'] = grouped['playlist_name'].apply(lambda names: '; '.join(names))\n",
    "    grouped.rename(columns={\n",
    "        'primary_artist': 'Artist',\n",
    "        'Track Name': 'Title',\n",
    "        'Album Name': 'Album',\n",
    "        'Duration (ms)': 'Duration_ms',\n",
    "        'is_liked': 'Is_Liked',\n",
    "        'is_top_songs': 'Is_Top_Songs'\n",
    "    }, inplace=True)\n",
    "    columns = ['Artist', 'Title', 'Album', 'Duration_ms', 'Playlists_Count', 'Playlists', 'Is_Liked', 'Is_Top_Songs']\n",
    "    grouped = grouped[columns]\n",
    "    grouped.sort_values(['Playlists_Count', 'Is_Liked', 'Artist', 'Title'], ascending=[False, False, True, True], inplace=True)\n",
    "    return grouped\n",
    "\n",
    "shopping_df = build_shopping_list(matched_df)\n",
    "if shopping_df.empty:\n",
    "    print('All playlist tracks already exist locally – no shopping list generated.')\n",
    "else:\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    shopping_path = SHOPPING_LIST_DIR / f'shopping_list_{timestamp}.csv'\n",
    "    shopping_df.to_csv(shopping_path, index=False)\n",
    "    print(f\"Shopping list saved to {shopping_path}\")\n",
    "    display(shopping_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc2cd00",
   "metadata": {},
   "source": [
    "## 8. Generate an orphaned-tracks list\n",
    "Highlight tracks that exist in `Downloaded/` but are not referenced by any current playlist snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd901248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_orphaned_tracks(matched_df: pd.DataFrame, library_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if library_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    playlist_keys = set(zip(matched_df['artist_canonical'], matched_df['title_canonical'])) if not matched_df.empty else set()\n",
    "    library_df = library_df.copy()\n",
    "    library_df['key'] = list(zip(library_df['artist_canonical'], library_df['title_canonical']))\n",
    "    mask = library_df['key'].apply(lambda key: key not in playlist_keys)\n",
    "    orphaned = library_df[mask].copy()\n",
    "    if orphaned.empty:\n",
    "        return pd.DataFrame()\n",
    "    orphaned.rename(columns={\n",
    "        'artist_raw': 'Artist',\n",
    "        'title_raw': 'Title',\n",
    "        'album_raw': 'Album',\n",
    "        'duration_ms': 'Duration_ms'\n",
    "    }, inplace=True)\n",
    "    columns = ['Artist', 'Title', 'Album', 'Duration_ms', 'file_path']\n",
    "    return orphaned[columns]\n",
    "\n",
    "orphan_df = build_orphaned_tracks(matched_df, library_index)\n",
    "if orphan_df.empty:\n",
    "    print('No orphaned tracks – every local track appears in at least one playlist snapshot.')\n",
    "else:\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    orphan_path = SHOPPING_LIST_DIR / f'orphaned_tracks_{timestamp}.csv'\n",
    "    orphan_df.to_csv(orphan_path, index=False)\n",
    "    print(f\"Orphaned-track report saved to {orphan_path}\")\n",
    "    display(orphan_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a77ff",
   "metadata": {},
   "source": [
    "## 9. Show Playlist Statistics\n",
    "Summarize key statistics about each playlist, including total tracks, matched tracks, missing tracks, and orphaned tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'matched_df' not in globals():\n",
    "    print(\"Run cells 1-7 to create 'matched_df' before generating playlist stats.\")\n",
    "elif matched_df.empty:\n",
    "    print('Playlist DataFrame is empty – load Spotify CSVs first.')\n",
    "else:\n",
    "    stats = (\n",
    "        matched_df\n",
    "        .groupby('playlist_name', dropna=False)\n",
    "        .agg(\n",
    "            Total_Tracks=('Track Name', 'size'),\n",
    "            Matched_Tracks=('file_path', lambda col: col.notna().sum()),\n",
    "            Liked_Snapshot=('is_liked', 'any'),\n",
    "            Top_Songs_Snapshot=('is_top_songs', 'any')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    stats['Missing_Tracks'] = stats['Total_Tracks'] - stats['Matched_Tracks']\n",
    "    stats['Percent_Complete'] = (stats['Matched_Tracks'] / stats['Total_Tracks'] * 100).round(1)\n",
    "    stats.sort_values(['Percent_Complete', 'playlist_name'], ascending=[False, True], inplace=True)\n",
    "\n",
    "    overall_total = int(stats['Total_Tracks'].sum())\n",
    "    overall_missing = int(stats['Missing_Tracks'].sum())\n",
    "    overall_matched = overall_total - overall_missing\n",
    "\n",
    "    missing_unique = (\n",
    "        matched_df[matched_df['file_path'].isna()]\n",
    "        .drop_duplicates(subset=['artist_canonical', 'title_canonical'])\n",
    "        .shape[0]\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Playlists analyzed: {len(stats)} | Tracks: {overall_total:,} | \"\n",
    "        f\"Matched: {overall_matched:,} | Missing: {overall_missing:,}\"\n",
    "    )\n",
    "    print(f\"Unique missing tracks across all playlists: {missing_unique:,}\")\n",
    "    display(stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57b5e3",
   "metadata": {},
   "source": [
    "## 10. Generate Playlists\n",
    "Build on these DataFrames to generate Innioasis Y1 playlist files (`.m3u8`) containing all the real tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df01e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYLIST_EXPORT_DIR = REPO_ROOT / \"generated_playlists\"\n",
    "PLAYLIST_RELATIVE_ROOT = Path(\"Music\")  # Adjust if your device expects a different root folder.\n",
    "PLAYLIST_EXPORT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "if 'matched_df' not in globals():\n",
    "    print(\"Run cells 1-7 to build 'matched_df' before exporting playlists.\")\n",
    "elif matched_df.empty:\n",
    "    print('matched_df is empty – load Spotify CSVs and re-run matching first.')\n",
    "else:\n",
    "    def playlist_filename(name: str) -> str:\n",
    "        safe = re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", (name or \"playlist\").strip())\n",
    "        safe = safe.strip('_') or \"playlist\"\n",
    "        return f\"{safe}.m3u8\"\n",
    "\n",
    "    exports = []\n",
    "    grouped = matched_df.groupby('playlist_name', sort=False)\n",
    "    for playlist_name, group in grouped:\n",
    "        resolved = group[group['file_path'].notna()].copy()\n",
    "        if resolved.empty:\n",
    "            continue\n",
    "        if 'Position' in resolved.columns:\n",
    "            resolved.sort_values('Position', inplace=True)\n",
    "        else:\n",
    "            resolved = resolved.sort_index()\n",
    "\n",
    "        playlist_path = PLAYLIST_EXPORT_DIR / playlist_filename(playlist_name)\n",
    "        lines = ['#EXTM3U']\n",
    "        for _, row in resolved.iterrows():\n",
    "            duration_sec = int(round(row['Duration (ms)'] / 1000)) if pd.notna(row.get('Duration (ms)')) else -1\n",
    "            artist = row.get('primary_artist') or row.get('Artist Name(s)') or 'Unknown Artist'\n",
    "            title = row.get('Track Name') or 'Unknown Title'\n",
    "            lines.append(f\"#EXTINF:{duration_sec},{artist} - {title}\")\n",
    "            device_path = (PLAYLIST_RELATIVE_ROOT / Path(row['file_path'])).as_posix()\n",
    "            lines.append(device_path)\n",
    "\n",
    "        playlist_path.write_text('\\n'.join(lines) + '\\n', encoding='utf-8')\n",
    "        exports.append({\n",
    "            'playlist_name': playlist_name,\n",
    "            'tracks_total': int(len(group)),\n",
    "            'tracks_written': int(len(resolved)),\n",
    "            'tracks_missing': int(group['file_path'].isna().sum()),\n",
    "            'output_file': playlist_path.relative_to(REPO_ROOT).as_posix()\n",
    "        })\n",
    "\n",
    "    if not exports:\n",
    "        print('No playlists had matched tracks – nothing exported.')\n",
    "    else:\n",
    "        summary = pd.DataFrame(exports)\n",
    "        summary.sort_values('playlist_name', inplace=True)\n",
    "        print(f\"Exported {len(summary)} playlists to {PLAYLIST_EXPORT_DIR}\")\n",
    "        display(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
